1. I'm going to talk about a Bayesian, noise-resilient algorithm for quantum amplitude estimation.

2. Quantum amplitude estimation is the problem of estimating the parameter a in this state, assuming we can prepare it. It's a fundamental routine with applications in Monte Carlo integration and expectation value estimation, making it very interesting for fields like finance or chemistry. This 'a' is usually called "the amplitude", despite being actually a probability; more specifically, the probability of collapsing this state to psi one upon measurement. There's a geometric interpretation that you probably associate with Grover's search algorithm. With this in mind, we can define a 'theta' that is determined by 'a'.

3. How can we solve this problem? The classical solution is to sample from the binomial distribution given by this state and take averages. This amounts to initializing and measuring the state repeatedly. On the other hand, if we can use some quantum operations, we can be a bit more creative. We can construct a quantum amplitude amplification operator, as used in Grover search; and if we apply phase estimation to it, we obtain an estimate for the amplitude. 

4. It has been shown that this approach has a quadratic speed-up relative to the classical case, just like Grover searching. Here the cost can be the number of applications of the initialization operator or the oracle. The best performing quantum algorithm has Heisenberg limited scaling, while the best performing classical algorithm follows the shot noise limit. 

5. The caveat is that the associated circuit, consisting of a ladder of controlled amplification operations followed by a Quantum Fourier transform, is very deep, very wide, and not very noise resilient. This feels particularly unfair because in Shor's algorithm, phase estimation is used to get an exponential advantage, whereas here we're going through the same trouble for a modest quadratic advantage. On the other hand, in quantum searching, we use amplification alone and get the same quadratic speed-up. This led to the question of whether we can't retain at least some advantage with less hardware demands. In recent years, alternative algorithms have been proposed that seek to do exactly this. They all have different degrees of quantum advantage, classical cost, noise resilience, and so on. Most of them are hardware friendly, in the sense that they require simpler circuits, but not lenient towards hardware flaws, in the sense that they don't account for noise.

6. For our approach, we consider the problem in these terms. We want to use amplification alone - the same type of circuits as in Grover search. We can use multiple circuits, but within this parametrized family. And we hope that this quantum-enhanced sampling will allow us to learn the amplitude more efficiently.

7. We are essentially estimating a parameter controlling a Bernoulli distribution. In the classical case, the probability is fixed; in the quantum case, we can change how that parameter controls the probability in a predictable way. The question is then: is that extra degree of freedom sufficient to bring a quantum advantage? For it to work, the amplified measurements have to be informative enough to compensate for the cost of amplification, plus some. The answer is yes; this type of measurements has been shown to be sufficient to retain the full quantum advantage. This ultimately means that the quantum enhancement comes exclusively from the ability to observe the system at different stages of amplification. 

8. How to use these measurements is an open question. Each answer brings a different quantum advantage, classical overhead and noise resilience.  Each answer brings a different quantum advantage, quantum cost offset, classical overhead and noise resilience. We can look at this as a data science problem: quantum circuits  produce otherwise unachievable experimental data. A typical scheme for such algorithms is represented here: we run an ensemble of Grover circuits and classically process the data. Often these protocols are adaptive, so each datum can be used to inform the following measurement. 
 
9. This is a general scheme for this type of iterative algorithms, relying on simpler quantum circuits along with classical processing and feedback. Almost all amplitude estimation algorithms follow this scheme. Additionally, in this formulation, QAE is largely similar to common characterization tasks associated with superconducting qubits. These tasks are related to precession dynamics, such as Larmor, Rabi and Ramsey oscillations. Similarly, in photonic quantum computing, Mach-Zehnder interferometry gives rise to a similar framework. 

10.  What changes is the classical component. There are approaches based on machine learning, on arithmetics, etc. One of these approaches is maximum likelihood estimation. We have a simple generative model for the data, so we can measure some circuits and then choose the parameter value most likely to have generated the observed data. This was one of the first proposals for simplified amplitude estimation, and the performance is remarkable, even when the circuits are performed based on very simple pre-determined heuristics. This approach is simple, parallelizable, and can easily accomodate noise models. However, it has some downsides too: it falls short of Heisenberg limited scaling, and it does not scale well with the model complexity. Complex noise models live in high dimensional spaces, where optimization does not fare well. 

11. One natural generalization of maximum likelihood estimation is Bayesian inference, where instead of optimizing, we integrate. As compared to optimization, integration is a more theoretically sound method, which can be robust even in high dimensional spaces. Not only does it improve scalability, it also conveys more information, which can be used to assess the merit of noise models and estimate the error, or more generally, any expectation value of a function of the parameters. 

12. This includes measures of merit for any choice of experimental design, which we can evaluate a measure of merit in a look-ahead scheme; hence, for every prospective  experiment, we can evaluate the expected utility. One problem with this is that the cost of evaluating the utility for each prospective experiment scales exponentially with the total number of experiments to be performed. 

13. This problem can be solved by exploiting a characteristic of Bayesian inference: we can choose whether to batch process the data, or to process them sequentially, as they arrive. The benefits are manifold. First,  we can opt for a locally optimal strategy whose cost does not scale exponentially with the number of experiments, but rather with the look-ahead size, which is a controllable parameter. While greedy strategies are not guaranteed to find the global optimum, it has been shown that they perform well, even with a unitary look-ahead. The second benefit is that we get up-to-date estimates at each iteration. And the third, which is related, is that we can leverage information as we collect it, meaning that at each point, we can use all available knowledge to inform the experimental controls. The fourth and last advantage is that...

14. ...This gives us the opportunity to introduce adaptivity, the tool that has been shown capable of achieving Heisenberg limited amplitude estimation. As a matter of fact, this same technique, Bayesian inference, has been used to achieve Heisenberg limited phase estimation without complex state preparation, and applied to other quantum characterization tasks, like sensing or magnetic field estimation. 

15. Adaptivity interests us as a resource for quantum advantage. It can have an effect similar to entangled state preparation in metrology, while easing up the requirements on the quantum hardware in exchange for classical processing. And in addition to that, it greatly simplifies the optimization task at hand.

16. Besides these considerations, others must be dealt with carefully. First, even with this simplification where the cost of one utility evaluation does not scale exponentially, the cost of optimizing it over possible choices does. And second, the statistical representation is crucial for the success of the inference protocol. Since an exact representation is not in general tractable, we must make approximations, which must not compromise the benefits that the framework brings. They should be compatible with adaptivity, cost-effective, and scalable. In quantum parameter estimation, more often than not, Gaussian assumptions are made. This works well for simple cases, but jeopardizes the scalability and robustness that makes Bayesian inference so rich. Not only can it misguide the experimental design, it is also bound to bias estimates once the problem becomes complex enough. Actually, in state of the art Bayesian inference software, developers focus the most on the statistical representation, which gives us an idea of how much simplistic approximations are not satisfactory in the general case. 

17. We propose to solve this by means of 2 strategies. On the statistical side, we employ an efficient sequential representation method that is highly parallelizable, compatible with adaptivity, scales well with the dimension, and provides figures of merit for the noise models, allowing for model comparison at barely any extra cost. On the optimization side, we employ problem tailored heuristics to cut down the processing costs. These two points strongly interact: the quality of the statistical representation determines how well the optimization can perform, and the optimization strategy determines how much strain is put on the statistical representation. This graph shows the results of our algorithm in a noiseless scenario. The tests are executed in a problem agnostic way.  It is shown numerically that it achieves Heisenberg-limited estimation. 

18. Here we juxtapose the results of our algorithm with others proposed in the literature, both for the ideal case  and in the presence of decoherence. Our algorithm is represented by the blue circles, whereas all others are alternative proposals. We can see that Bayesian amplitude estimation is competitive in both cases. The slope trend is quite favorable, as is the vertical cost offset. The latter is a metric that is not contemplated in complexity considerations, and as such is often disregarded in the literature, but is relevant in practice, especially having near term demonstrations of quantum advantage in mind. Another often ignored performance metric is the classical processing cost, which is entirely orthogonal to this type of key plots. This is why we are concerned with efficient classical processing, because "good" performances can hide a prohibitive cost overhead. We can see that despite our cost saving measures, the crucial performance in terms of the fundamental costs as represented in this graph is solid. We worked towards maximal preservation of the quantum advantage, even if for the sake of practical viability, we are concerned with other types of costs as well.  

19. And finally, another key aspect that is frequently looked over when looking at near term QAE algorithms is the noise resilience. So-called "hardware friendly" algorithms, including those with ideal scaling, can actually behave disastrously in the presence of noise; underestimating the error, and even failing to terminate. In graphs like these, which are usually used for benchmarking purposes, we consider the cumulative circuit length across all circuits; but the maximum circuit depth is very relevant as well, especially given that most algorithms tend to impose unchecked depth growht across iterations, and provide no way of adjusting to realistic scenarios where the coherence time is finite. This is what we test on the right: how algorithms fare in the presence of decoherence. Our algorithm performs reliably even for higher numbers of queries, corresponding to longer executions and higher precision, whereas others with similar costs come to a stall due to the noise. Among the other algorithms, the ones that don't show this trend are those which have higher cost offsets. This is because the're using a higher than necessary absolute amount of quantum resources, which inflate the cost in such a way that they spend more quantum resources  to achieve a fixed error. Once we are working in the same error regimen, the behavior will become unstable as well. 


20. So in short, we propose a method that is capable of attaining Heisenberg limited estimation; is highly customizable and capable of negotiating trade-offs across the multiple costs involved; is parallelizable and scalable; and is resilient to noise. Our ultimate aim is to bring amplitude estimation closer to a practical advantage. 



